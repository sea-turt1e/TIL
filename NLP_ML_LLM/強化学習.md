# 強化学習
## 基本概念
- エージェント（=LLM）が
- 環境（=プロンプト）に対して
- 行動（=テキストを生成）をとり
- 報酬（=人間の評価スコア）を受け取る

## 強化学習をLLMの文脈で例えると
- エージェント=LLM
- 状態（State）= これまでの文脈（プロンプト+生成済みテキスト）
- 行動（Action）= 次のトークンを選ぶ
- 方策（Policy）= モデルの確率分布π_θ
- 報酬（Reward）= 報酬モデルのスコア

## PPOの核心
- 大きく変えすぎない
- 単純に報酬が高くなるように更新し続けるとモデルが報酬ハッキングを起こす
```
報酬ハッキングの例:
  報酬モデル: 「長い文章は詳しそうで高スコア」と学習
  LLM: 「じゃあとにかく長く書けばいい！」
  → 無意味な繰り返しで長文を生成
  → 報酬は高いが品質は最悪
```

![alt text](<image/スクリーンショット 2026-02-28 7.15.38.png>)

- 直感的な意味
```
報酬が高かった → でも20%以上は変えない
報酬が低かった → でも20%以上は変えない

→ 少しずつ安全に改善していく
```