# 学習レシピとアライメント

## Pretrain: 大規模コーパス構築
- LLMのデータソースは複数のソースが使われる。
- 典型的な構成は以下
| ソース                | 特徴          | 比率（目安） |
| ------------------ | ----------- | ------ |
| Common Crawl / Web | 最大規模、ノイズ多い  | 50〜70% |
| Wikipedia          | 高品質、多様なトピック | 5〜10%  |
| 書籍・論文              | 長文・高品質      | 10〜20% |
| コード (GitHub)       | 推論能力向上に寄与   | 5〜10%  |
| ニュース・Q&A           | 事実性が高い      | 5〜10%  |

### Dedup(De-duplication) 重複削除
- 重複データを削除しないとモデルが特定のパターンを丸暗記/学習する恐れがある
- 3レベルのdedupがある
  - 完全一致（Extract）
    - Hash（MD5/SHA）で一致を検出
  - 近似一致（Fuzzy）
    - MniHash + LSH（局所性鋭敏型ハッシュ）
    - Jaccard類似度を計算し、閾値以上なら削除
  - 意味的一致（Semantic）
    - 文書埋め込み -> コサイン類似度で比較
    - 埋め込みが近ければ一方を削除
- RefinedWebの事例: CommonCrawlの約5兆トークンから90%を削除し、高品質な500億トークンを抽出。「引き算の哲学」が重要

### フィルタリング
#### ヒューリスティックフィルタ
```
① URL/ドメインフィルタ:
   スパム・アダルトサイトのドメインをブロックリストで除外

② 言語フィルタ:
   fastText/langdetect で日本語確率 < 0.8 のものを除外

③ テキスト長フィルタ:
   短すぎる文書（例: 50トークン以下）を除外
   短すぎる行が多い文書（ナビゲーションメニューなど）を除外

④ 記号比率フィルタ:
   記号・数字の割合が異常に高い文書を除外
   例: "!!!!!買って!!!! 5000円→500円!!!!" → 除外

⑤ 重複行フィルタ:
   同じ行が繰り返されている文書を除外
   例: フッターが大量コピーされたWebページ
```

#### モデルベースフィルタ（高精度・高コスト）
```
① 品質スコアリング:
   高品質文書（Wikipedia等）で学習した分類器で品質スコアを計算
   例: Kenlm perplexityが高すぎる文書は低品質として除外

② 有害コンテンツフィルタ:
   ヘイトスピーチ、差別的表現を分類モデルで除外

③ PII（個人情報）除去:
   電話番号、メールアドレス、住所などを正規表現 + NERで除去
```

#### 日本語固有のフィルタリング
```
- Unicode正規化（NFKC）: 全角・半角の統一
- 文字化けの除去: □□□ のような文字
- Ruby（ルビ）テキストの処理: HTMLから正しく抽出
- 縦書きテキストの変換
- 機種依存文字の処理
```

#### データミキシング比率の設計
単純にすべてのデータを混ぜるのではなく、比率を調整することが重要です:

```python
# データミキシングの例
dataset_weights = {
    "web": 0.60,        # CommonCrawl（日本語フィルタ済み）
    "wikipedia": 0.10,  # 日本語Wikipedia
    "books": 0.15,      # 書籍データ
    "code": 0.08,       # GitHub（日本語コメント付き）
    "qa": 0.07,         # Yahoo知恵袋、Stack Overflow
}
# 高品質ソースを過剰にサンプリング（Upsampling）することも有効
```

## SFT
- SFTは多様なタスクをカバーする必要あり
```
タスクカテゴリの例:

① 質問応答
   "東京の人口は？"

② 要約
   "以下の文章を3行に要約してください：..."

③ 創作・生成
   "桜をテーマにした俳句を作ってください"

④ コード生成
   "PythonでFizzBuzzを実装してください"

⑤ 翻訳
   "以下を英語に翻訳してください：..."

⑥ 推論・分析
   "AとBの違いを説明してください"

⑦ 指示に従う（命令型）
   "箇条書きで5つ挙げてください"

⑧ 安全性・拒否
   "この質問には答えられません、なぜなら..."
```
- タスクが偏ると、モデルが特定のタスクに過学習する。

### フォーマット設計とテンプレート
- SFTは統一されたフォーマットに変換して学習される

#### 基本的な指示フォーマット
- Alpaca方式
```
### 指示:
{instruction}

### 入力:
{input}  ← 省略可能

### 応答:
{output}
```

- Chat形式（ChatML）
```
<|im_start|>system
あなたは親切なアシスタントです。
<|im_end|>
<|im_start|>user
東京の天気を教えて
<|im_end|>
<|im_start|>assistant
今日の東京は晴れです。
<|im_end|>
```

- Llamaのデフォルト形式
```
[INST] <<SYS>>
あなたは親切なアシスタントです。
<</SYS>>

東京の天気を教えて [/INST]
今日の東京は晴れです。
```

### フォーマット設計の重要な点
- 損失マスキング
  - プロンプト部分[INST]は損失計算から除外する
  - 応答部分（モデルの出力）だけ損失を計算
  - つまり入力の暗記ではなく、応答の仕方を学ぶ
- システムプロンプトの活用
  - モデルのペルソナ・制約を統一
  - 例: 日本語で回答してください
- 一貫性
  - 訓練中に使うフォーマット=推論時に使うフォーマット
  - テンプレートが変わると大幅に性能が低下

## DPO/RLHF: Preferenceデータの設計
### ペアワイズ比較
- 同じプロンプトに対する2つの譜等を比較する形式
```
プロンプト: "機械学習を初心者に説明してください"

応答A（chosen）:
  "機械学習とは、コンピュータがデータから
   自動的にパターンを学習する技術です。
   例えば..."

応答B（rejected）:
  "機械学習は統計的手法を用いた..."
  （専門用語が多く難しい）

アノテーター: A > B と判断
```

#### データフォーマット
```
{
  "prompt": "機械学習を初心者に説明してください",
  "chosen": "機械学習とは、コンピュータが...",
  "rejected": "機械学習は統計的手法を用いた..."
}
```

### Ranking形式
3つ以上の複数の応答を順位づけする方式
```
プロンプト: "東京のおすすめ観光地は？"

応答A: 浅草、東京タワー、新宿御苑など詳しく説明
応答B: 東京タワーだけを紹介（情報量少ない）
応答C: 渋谷と原宿を紹介（普通）
応答D: 「わかりません」（不適切な拒否）

ランキング: A > C > B > D
```

### Ranking -> Parewiseへの変換
```
A > C > B > D から生成されるペア:
  A > C
  A > B
  A > D
  C > B
  C > D
  B > D
→ 4応答で6ペアが生成される（n(n-1)/2）
```

### 比較表
| 観点          | Pairwise    | Ranking       |
| ----------- | ----------- | ------------- |
| アノテーションの容易さ | 簡単（2択）      | やや難しい（順位付け）   |
| データ効率       | 1ペア = 1データ点 | 1ランキングで複数ペア生成 |
| 一貫性         | 推移性が保証されない  | 順位付きで一貫性が高い   |
| 代表的な使用      | DPO         | RLHFの報酬モデル学習  |

## オフライン vs オンラインのpreference収集
### オフライン（static）設計
事前にpreferenceデータを収集し、固定されたデータセットで学習
```
フロー:
① データ収集（人間アノテーター or 既存の強力なモデル）
② データセットを固定
③ DPO/RLHF学習
④ 評価

特徴:
- シンプルで再現性が高い
- 一度収集したデータを繰り返し使える
- モデルが学習しても「学習中のモデル」の応答は使わない

代表的なデータセット:
- Anthropic HH-RLHF
- OpenAI WebGPT
- UltraFeedback（合成）
```

### オンライン（Dynamic）設計
学習中のモデルからサンプリングしながらpreferenceを収集
```
フロー:
① 現在のモデルπθでプロンプトから応答を生成
② 生成した応答にpreference（or 報酬）を付与
③ そのデータで即時学習
④ ①に戻る（繰り返し）

特徴:
- モデルが学習するにつれてデータも更新される
- より効率的に改善できる
- 実装が複雑（報酬モデルや評価が必要）
- On-policy DPO と呼ばれることもある
```

### 比較
| 観点     | オフライン            | オンライン                |
| ------ | ---------------- | -------------------- |
| 実装の複雑さ | シンプル             | 複雑                   |
| データ品質  | 固定（古くなる可能性）      | 常に最新のモデルに対応          |
| コスト    | 収集コストが前払い        | 継続的にコストが発生           |
| 分布のズレ  | あり（学習前後でモデルが変わる） | ほぼなし                 |
| 代表例    | DPO（基本形）         | RLHF、GRPO、Online DPO |

### 分布のズレ
```
データ収集時のモデル（SFT済み）:
  "東京の天気は晴れです" → 60%の確率で生成

DPO学習後のモデル:
  "東京の天気は晴れです" → 90%の確率で生成

→ 収集時のデータがもはやモデルの「実際の振る舞い」を
  反映していない
→ オフラインデータは古くなる
```

### 全体のパイプライン
```
① Pretrain
   大規模コーパス（dedup + フィルタリング）
   → 言語知識・世界知識の習得
          ↓
② SFT
   多様な指示追従データ（テンプレート統一）
   → 「指示に従うAI」への変換
          ↓
③ Preference収集
   Pairwise比較 or Ranking形式
   オフライン（固定データ）or オンライン（動的収集）
          ↓
④ DPO or RLHF
   preferenceデータでアラインメント
   → 安全性・有用性・正確性の向上
```

## RLHF（Reinforcement Learing from Human Feedback）
人間のフィードバックを使って強化学習でモデルを改善する手法
```
① SFT済みモデルを用意

② 報酬モデル（Reward Model）の学習
   preferenceデータ（A > B のペア）を使って
   「良い応答に高スコア、悪い応答に低スコア」を出す
   分類器を訓練する
③ 強化学習（PPO）でモデルを更新
現在のモデルから応答を生成
↓
報酬モデルがスコアをつける
↓
スコアが高くなるようにPPOで重みを更新
↓
繰り返す
```

- 重要点: ③で「現在のモデルから応答生成してスコアをつける」というループがオンポリシーの本質

### Rankingが必要な理由
報酬モデルはスコアを出力する。このスコアを学習するには複数の応答の相対的な優劣が必要

```
プロンプト: "量子力学を説明して"

応答A: 詳しく正確に説明（スコア: 0.9）
応答B: 普通の説明（スコア: 0.6）
応答C: 誤りが含まれる（スコア: 0.2）
応答D: 無関係な回答（スコア: 0.0）

ランキング: A > B > C > D

報酬モデルはこの順序を正しく再現できるように学習する
→ 2択のペアワイズでは「どれくらい良いか」の
  スケールが学習しにくい
→ 複数を並べたランキングのほうが相対的な品質を
  効率よく学習できる
```

## DPO（Direct Preference Optimization）
報酬モデルも強化学習も使わず、preferenceデータから直接モデルを更新する手法

### 革新的なアイデア
RLHFの数式を変形すると、報酬μデルを明示的に学習しなくてもモデルの確立分布の比率として報酬を表現できることが示された。

```
RLHFの考え方:
  preferenceデータ → 報酬モデルを学習
  報酬モデル → PPOでモデルを更新

DPOの考え方:
  preferenceデータ → 直接モデルを更新
  （報酬モデルは暗黙的に内包されている）
```

![alt text](<image/スクリーンショット 2026-02-28 6.41.49.png>)

### 直感的な意味
```
良い応答の確率を上げる:
  π_θ(y_w | x) が高くなるように更新

悪い応答の確率を下げる:
  π_θ(y_l | x) が低くなるように更新

ただし参照モデルから離れすぎない:
  SFTで学んだ能力を忘れないよう、βで制御
```

### Pairwiseで十分な理由
DPOはRankingを必要とせず A>Bの2択ペアだけで学習できる
```
損失関数が「chosen vs rejected の確率の差」を直接最適化するため、
スコアの絶対値が不要

RLHF: 「Aのスコアは0.9、Bのスコアは0.6」が必要
DPO:  「AはBより良い」という事実だけで十分
```
## 比較表
| 観点              | RLHF（PPO）           | DPO                           |
| --------------- | ------------------- | ----------------------------- |
| 必要なモデル数         | 4つ（SFT、報酬、学習中、参照）   | 2つ（学習中、参照）                    |
| 学習の複雑さ          | 複雑（強化学習ループ）         | シンプル（教師あり学習と同じ枠組み）ai-scholar​ |
| 学習の安定性          | 不安定になりやすい           | 安定life-key.tistory​           |
| Preferenceデータ形式 | Ranking（複数応答）       | Pairwise（2択）note​             |
| オフライン/オンライン     | 基本的にオンライン           | 基本的にオフライン                     |
| 性能              | 高い（上限が高い）           | 同等以上のことも多いnote​               |
| 代表モデル           | InstructGPT、ChatGPT | LLaMA系、Mistral系               |


### RLHFとDPOの必要なモデル数
- RLHFでは同時に4つのモデルをGPUに乗せる必要がある
```
① SFTモデル（参照モデル、π_ref）
   └── 「人間らしい言語能力を保つ基準」
   └── 固定（重みは更新しない）
   └── PPO学習中ずっとKL制約の計算に使う

② 報酬モデル（Reward Model）
   └── 「この応答は何点か？」を計算
   └── 固定（RLHFの②で学習済み）
   └── 各ステップで応答にスコアを付ける

③ 学習中のモデル（π_θ）
   └── 実際に更新されるモデル
   └── プロンプトに対して応答を生成する

④ 価値モデル（Value Model / Critic）
   └── 「この状態（文脈）の将来の期待報酬は何点か？」を推定
   └── PPOのアドバンテージ計算に必要
   └── 通常SFTモデルと同サイズで初期化
```
- なぜ4つ全部必要か: 全部同時に動くから
```
各学習ステップで:
  ③学習中モデル → テキストを生成（Forward）
  ②報酬モデル → 生成テキストにスコアを付ける
  ①参照モデル → 現在のモデルが参照モデルから
               どれだけズレているか（KL divergence）を計算
  ④価値モデル → アドバンテージを計算

→ 4つが同時に動く
→ GPUメモリが4倍必要
```

- DPOでは2モデルで済む
```
① 参照モデル（π_ref = SFTモデル）
   └── 固定
   └── DPOの損失関数の分母に使う

② 学習中のモデル（π_θ）
   └── 更新される

なぜ2つで済むか:
  報酬モデルが不要（損失関数に内包されている）
  価値モデルが不要（強化学習ではないため）
```

## オフラインとオンラインの違い
- 学習データが学習中に変化するかどうか。
- 変化しない=オフライン=static
- 変化する=オンライン=dynamic

## オンポリシー
- ポリシー: どんな状態でどんな行動を取るかの確率分布、つまりLLMそのもの
- π_θ (トークン∣文脈)=次のトークンの確率

### オンポリシーの定義
現在学習中のポリシー（モデル）が生成したデータで学習すること
```
オンポリシー（RLHF/PPO）:
  ループの各ステップで...

  現在のモデルπθ（最新）でテキストを生成
        ↓
  報酬モデルがスコアを付ける
        ↓
  このデータでπθを更新 → πθ+1
        ↓
  πθ+1で新しくテキストを生成（古いデータは捨てる）
        ↓
  繰り返す

  → 常に「今のモデルの行動」に対して学習
```

## オフボリシー
従来の学習。学習データは固定
```
オフポリシー（DPO）:
  過去に収集したデータ（別のモデルが生成したかも）を使って学習

  SFT済みモデルが生成したデータ
        ↓
  固定データセットとして保存
        ↓
  学習中のモデルがどれだけ変わっても同じデータを使い続ける

  → 「過去のモデルの行動」に対して学習
```

## 学習の安定性
### RLHFが不安定な理由
複数の不安定要素が連鎖するから

1. 報酬ハッキング
```
報酬モデルの欠点:
  有限のデータで学習した「人間の好みの近似」に過ぎない
  → 完璧ではない

PPOがその欠点を突く:
  「報酬モデルを騙すような応答」を生成し始める
  → 人間には不自然でも報酬が高い応答が増える

例:
  "日本で一番高い山は？"
  → 「富士山です。富士山は日本一高い山です。
      富士山の標高は3776mです。富士山...（無限ループ）」
  → 報酬モデルはなぜか高スコアを付ける
```

2. PPOのハイパーパラメータ感度
```
学習率が高すぎる → モデルが崩壊
学習率が低すぎる → 改善が遅い
εの調整が難しい → 更新幅の制限が強すぎ/弱すぎ
KL係数の調整が難しい → 参照モデルから離れすぎ/近すぎ
```

3. 4モデルの相互依存
```
学習率が高すぎる → モデルが崩壊
学習率が低すぎる → 改善が遅い
εの調整が難しい → 更新幅の制限が強すぎ/弱すぎ
KL係数の調整が難しい → 参照モデルから離れすぎ/近すぎ
```

### DPOが安定な理由
教師あり学習（SFT）と同じ枠組みで動くから
```
DPOの学習ループ:
  バッチを取り出す（固定データ）
    ↓
  損失を計算（前向き計算）
    ↓
  勾配を計算（逆伝播）
    ↓
  重みを更新

→ これはSFTとまったく同じ流れ
→ 強化学習ループ・報酬ハッキング・
  複数モデルの相互依存がない
```

### 比較表
| 不安定要因      | RLHF             | DPO        |
| ---------- | ---------------- | ---------- |
| 報酬ハッキング    | 起きやすい            | 報酬モデル自体がない |
| ハイパーパラメータ数 | 多い（PPOのε、KL係数など） | 少ない（βのみ）   |
| モデル間の相互依存  | 4モデルが相互依存        | 2モデルで独立    |
| 学習ループの複雑さ  | 強化学習ループ          | 通常の勾配降下    |
| 数値的な不安定性   | RL特有の不安定さ        | SFTと同等の安定さ |

## 全体の関係図
```
RLHF（PPO）:
  ┌── SFTモデル（参照、固定）
  ├── 報酬モデル（固定）
  ├── 学習中モデル（更新）  ← オンポリシーで自分が生成したデータで学習
  └── 価値モデル（更新）
  → 高性能だが複雑・不安定・メモリ大量消費

DPO:
  ┌── 参照モデル（SFT済み、固定）
  └── 学習中モデル（更新）  ← オフポリシーで固定データで学習
  → シンプル・安定・メモリ少ない
    ただしオフポリシーによる分布ズレの問題あり
```