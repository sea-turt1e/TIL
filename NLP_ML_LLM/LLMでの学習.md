
## LLMにおけるSFTとは
- 入力と出力（正解）のペアを大量に用意して微調整する。
- データ形式
```
{
  "input": "東京の天気を教えて",
  "output": "今日は晴れです。"
}
```

### 従来のFineーtuningとの違い
- 最も重要な違いは「何を教えるか」
#### 従来のFinetuning
- 目的: 分類や回帰など特定タスクの適用
- データ形式: ラベル付きデータ（感情ラベルなど）
- 出力層: タスク用の出力層を付け替える
- 損失計算: タスク雇用の損失（例: Cross Entropy Loss）
- 代表例: BERTに感情分類機を追加
```
入力: "この映画は最高だった"
↓
[CLS]トークンの表現
↓
分類器（新しく追加した層）
↓
出力: ポジティブ / ネガティブ
```
- モデルの一部（出力層）を付け替える
- 「このテキストは何クラスか」を教える

#### SFT
- 目的: 指示に従う能力の付与（プロンプト通りに振る舞う）
- データ形式: プロンプト + 応答のペア
- 出力層: 言語モデルの出力層はそのまま
- 損失計算: トークンタインのCross Entropy
- 代表例: GPTにQAデータで会話能力を付与
```
入力（プロンプト）: "東京の天気を教えて"
↓
LLM全体
↓
出力（トークンを逐次生成）: "今日の東京は晴れです。"
```
- 出力層は変えない
- 「この入力に対してどう応答すべきか」という振る舞い全体を教える

#### SFTがLLMに必要な理由
- 事前学習（pre-training）だけでは不十分な場合がある。
```
事前学習後のモデルに「東京の天気は？」と聞くと...

"東京の天気は？ 東京の気象情報... 天気予報... 東京都..."
← 次の単語を予測するだけで、「質問に答える」という概念がない
```

- SFTにより
```
SFT後のモデルに「東京の天気は？」と聞くと...

"今日の東京は晴れです。" ← 質問に適切に答えられる
```

- SFTは「次の単語予測マシン」を「指示に従うアシスタント」に変換するステップ

#### LLMの学習パイプラインの全体での位置付け
```
① 事前学習（Pre-training）
   大量テキストで「言語の知識」を習得
   （次の単語を予測するだけ）
          ↓
② SFT（Supervised Fine-Tuning）  ← ここ
   プロンプト＋応答ペアで「指示に従う能力」を付与
   （ChatGPTのベースになる段階）
          ↓
③ RLHF / DPO
   人間のフィードバックで「より好ましい応答」を学習
   （安全性・有用性の向上）
```

#### SFTの損失計算の特徴
LLMのSFTでは、応答部分のトークンのみに損失を計算します

```
入力全体:
"[プロンプト] 東京の天気は？ [応答] 今日は晴れです。"

損失計算:
 ←        無視         →  ← ここだけ損失計算 →
"[プロンプト] 東京の天気は？ [応答] 今日は晴れです。"
```

## 位置表現: RoPE vs ALiBi
- なぜ位置情報が必要か
- Transformerのself-attentionは単語の順序情報を持たないため、位置情報を別途与える必要がある。
```
"猫が犬を追う" と "犬が猫を追う"
→ Attentionだけでは区別できない
→ 位置情報が必要
```

| 観点             | RoPE         | ALiBi                   |
| -------------- | ------------ | ----------------------- |
| 仕組み            | Q・Kを回転       | Attentionスコアに距離ペナルティを引く |
| 追加パラメータ        | なし           | なし                      |
| 計算コスト          | やや高い（回転行列）   | ほぼ無視できる（加算だけ）           |
| 訓練長さ内の性能       | 高いlinkedin​  | やや劣る                    |
| 長文外挿（コンテキスト拡張） | 苦手           | 得意mbrenndoerfer+1       |
| 代表モデル          | LLaMA, Gemma | BLOOM, MPT              |

- 絶対位置とは違い、相対位置はトークン間の向きはなくなる
- ただし符号（+=）はわかるので、順番はわかる。
```
位置3のQ（3θ回転済み）
      ↑（角度3θ）

位置1のK（1θ回転済み）
    ↗（角度1θ）

内積 = 2つのベクトルの「なす角」に依存
     = (3θ - 1θ) = 2θ の情報だけ
     = 相対距離2の情報だけ
```

### 絶対位置
```
猫 か 犬 を 追 う
0 1 2 3 4 5 <- 絶対位置（indexのようなもの）
```



## Mixture-of-Experts(MoE)
- 全ての入力に同じ全重みを使わず。入力に応じて「専門家（Expert）」を選んで計算する。
```
通常のFFN:
入力 → FFN（全パラメータを使用）→ 出力

MoEのFFN:
入力 → Router → Expert2, Expert5を選択
                Expert2 ─→ 出力2 ─┐
                Expert5 ─→ 出力5 ─┼→ 加重平均 → 出力
                （他のExpertは動かない）
```
- 病院の例え: 患者が来たら受付（Router）が「消化器内科（Expert2）と外科（Expert5）に行って」と振り分ける。他の科の先生はその患者のために動かない
- MoEの利点:パラメータ数（モデルの知識量）と計算量を切り離せるのがMoEの最大の利点

### Routerの学習
- Routerは学習可能な線形変換で、どのExpertに何の入力を送るかを習得する
```
学習後のRouterの例（推測）:
"数学の問題" → Expert3, Expert7を選択（数学専門のExpert）
"コードの質問" → Expert1, Expert5を選択（コード専門のExpert）
"翻訳タスク" → Expert2, Expert6を選択（言語専門のExpert）
```
- ただしRouterが何を学習したかは完全には解釈できない=ブラックボックス的な面もある

### Load Balancing問題
- Routerが同じExpertばかり選ぶCollapse（崩壊）が起こる可能性がある
- ROuterがExpert1ばかり選ぶ
  - Expert1だけ学習が進み、他のExpertが選ばれなくなる（学習もされなくなる）
  - これを防ぐために Auxilarty Loss(補助損失)を加えて、均等に選ばれるよう学習する

### まとめ
| 技術             | 解決する問題           | 仕組み                  | 代表モデル             |
| -------------- | ---------------- | -------------------- | ----------------- |
| RoPE           | 位置表現の質           | Q・Kを回転させて相対位置を表現     | LLaMA, Gemma      |
| ALiBi          | 長文外挿             | Attentionスコアに距離ペナルティ | BLOOM, MPT        |
| MQA/GQA        | KVキャッシュのメモリ削減    | K・VをヘッドでShare        | LLaMA2, Mistral   |
| FlashAttention | Attentionの速度・メモリ | タイル分割でSRAMを活用        | ほぼすべての現代LLM       |
| MoE            | パラメータ数と計算量の分離    | 入力に応じてExpertを選択      | Mixtral, DeepSeek |





