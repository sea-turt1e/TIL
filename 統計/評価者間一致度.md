# 評価者間一致度
複数の評価者がどれだけ一貫して評価しているか測る指標

## Cohen's Kappa（コーエンのカッパ）
2人の評価者の一致度を測る基本的な指標
![alt text](<image/スクリーンショット 2026-02-10 9.52.57.png>)

### Weighted Kappa
不一致どの程度を考慮
- 例えば1~5の評価で
  - Aが1、Bが5なら深刻な不一致
  - Aが1、Bが2なら軽微な不一致

## Fleiss Kappa（フライスのカッパ）
3人以上の評価者の一致を測る。基本の考え方はCohens kappaと同じ

## Krippendorff's Alpha（クリッペンドルフのα）
2人以上、かつ欠損地があっても対応可能

## LLM評価での具体的な使い分け
- ケース1: 主観評価（2人の専門家）
  - タスク: LLMの応答品質を「Excellent/Good/Fair/Poor」で評価
  - 推奨: Weighted Cohen's Kappa（順序関係を考慮）
- ケース2: 安全性評価（3人のアノテーター）
  - タスク: 有害性を「Safe/Unsafe」で評価
  - 推奨: Fleiss' Kappa（3人以上、二値分類）
- ケース3: KGトリプルアノテーション
  - タスク: 複数人が「Correct/Wrong/Unsure」を評価、評価数が人により異なる
  - 推奨: Krippendorff's Alpha（欠損データに対応）
​
## 一致度の改善方法
- 低い一致度（κ < 0.4）の場合
  - 評価ガイドラインの改善：境界ケースの例を追加
  - 評価者トレーニング：不一致の原因を分析し、再訓練
  ​- 評価基準の明確化：曖昧な基準を具体化
  - パイロット評価：本評価前に小規模テストで一致度を確認

## 目標とする一致度
![alt text](<image/スクリーンショット 2026-02-10 10.29.49.png>)