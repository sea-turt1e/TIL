# エントロピー
- 不確実性や選択肢の多さを指す
- つまり結果がどれだけ予測しにくいかを指す
- 直感的な理解
  - 低いエントロピー = 確実（予測しやすい）
    - 明日も太陽は東から昇る → ほぼ確実
    - 偏ったサイコロ（常に6が出る）→ 予測できる
  - 高いエントロピー = 不確実（予測しにくい）
    - 明日の天気は？ → 晴れ、曇り、雨など複数の可能性
    - 公正なサイコロ → 1〜6のどれが出るかわからない
- 例: コイン投げ
  - パターンA: 両面とも表のコイン
    - 結果: 必ず表
    - 不確実性: ゼロ（予測が100%確実）
    - エントロピー = 0
  - パターンB: 普通のコイン
    - 結果: 表50%、裏50%
    - 不確実性: 最大（どちらが出るかわからない）
    - エントロピー = 1.0 bit（最大値）
  - パターンC: 偏ったコイン
    - 結果: 表90%、裏10%
    - 不確実性: 中程度（だいたい表だが、たまに裏）
    - エントロピー = 0.47 bit
    -  ![alt text](<image/スクリーンショット 2026-02-10 10.58.34.png>)

# クロスエントロピー
- エントロピーは1つの分布の不確実性を表すのに対し、クロスエントロピーは2つの分布のずれを表す
  - エントロピー: 1つの分布の不確実性
    - 天気予測「晴れ50%, 雨50%」　最も不確実（高エントロピー）
  - クロスエントロピー: 2つの分布のずれ
    - 予測が正解からどれだけ離れてるか
    - 正解と予測の2つを比較
    - 損失関数として使用。
      - 機械学習分類問題ではone-hotエンコーディングなので、元々のクロスエントロピーの指揮より簡略化できる。
      - ![alt text](<image/スクリーンショット 2026-02-10 11.09.02.png>)

# KLダイバージェンス
- 2つの確率分布QとPがどれだけ異なるかを測る指標
- 別名: カルバックライブラー情報量。KL距離
- ![alt text](<image/スクリーンショット 2026-02-10 11.36.47.png>)
- 直感的な意味
  - 真の分布Pを使うべきところで、近似分布Qを使った時に生じる情報の無駄
- 具体例
- ![alt text](<image/スクリーンショット 2026-02-10 11.39.50.png>)

## LLMでの応用
### RHLFのときのKLペナルティ
- ペナルティがないと報酬を最大化する方向へ暴走
- 天気の場合「晴れしかありません！99.9%」みたいな
- トークンごとにKLを計算して、新しいモデルがトークンk_tを出力する確率と、元のモデルがトークンk_tを出力する確率の差が大きい場合、元のモデルと大きく異なる
  - 元のモデルと大きく異なる場合は大きなペナルティとして計算される

### DPO
- 損失関数に直接KLダイバージェンスを組み込む
- ![alt text](<image/スクリーンショット 2026-02-10 11.54.01.png>)

### 知識蒸留
- 知識蒸留のKLダイバージェンス
  - 目的: 生徒モデルの出力分布を教師に近づける
  - 知識: 教師の確率分布が持つ「不確実性の構造」
  - Temperature: 分布を滑らかにして細かい情報を伝える
  - 損失関数: KL損失（ソフト）+ CE損失（ハード）
  - 効果: 小さなモデルで大きなモデルの性能に近づける
- 具体的な応用:
  - LLMの小型化（70B → 7B）
  - エッジデバイスへのデプロイ
  - 推論速度の向上（コスト削減）



